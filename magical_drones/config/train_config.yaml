trainer:
  max_epochs: 1
  precision: "bf16-true"
  # accumulate_grad_batches: 4 can't use grad_accumulation with manual optimizers
model:
  width: 128
  height: 128
  lr: 0.0003
  lambda_cycle: 10
data:
  data_link: "czarna-magia/mag-map"
  batch_size: 4
  transform: null # problem with passing transforms in that way